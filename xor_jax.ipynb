{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dramatic-preserve",
   "metadata": {},
   "source": [
    "# an intro to jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-cleaners",
   "metadata": {},
   "source": [
    "jax is a high performance python library for ML research. It includes an updated version of autograd, a jit compiler for GPUs and TPUs, and is apparently really good at vectorising things. Its not an official Google product, but it is maintained by them so its probably not gonna go anywhere soon. Generally, you can use a numpy-esque syntax to write your code, so it should be quite familiar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-change",
   "metadata": {},
   "source": [
    "we are gonna use two references for this .ipynb:\n",
    "        \n",
    " * https://github.com/google/jax\n",
    "        \n",
    " * https://colinraffel.com/blog/you-don-t-know-jax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfactory-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electric-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "# referring to numpy as np is so last year, now np as jax\n",
    "import jax.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amateur-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-airline",
   "metadata": {},
   "source": [
    "we're gonna make an example (just as in one of the above links) to make an xor gate\n",
    "\n",
    " * 0 0 -> 0\n",
    " \n",
    " * 0 1 -> 1\n",
    " \n",
    " * 1 0 -> 1\n",
    " \n",
    " * 1 1 -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-manchester",
   "metadata": {},
   "source": [
    "whats interesting, is we can build up an entire NN using just jax syntax (well, i suppose you could do that with numpy to?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "strategic-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes our network's output\n",
    "def net(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    # so this is a hidden layer\n",
    "    # with a tanh activation\n",
    "    hidden = np.tanh(np.dot(w1, x) + b1)\n",
    "    # we then have our final layer\n",
    "    # we are using our sigmoid here\n",
    "    return sigmoid(np.dot(w2, hidden) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closed-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "# standard stuff\n",
    "def loss(params, x, y):\n",
    "    out = net(params, x)\n",
    "    cross_entropy = -y * np.log(out) - (1 - y)*np.log(1 - out)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intended-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for testing whether the net produces the correct\n",
    "# output for all possible inputs\n",
    "def test_all_inputs(inputs, params):\n",
    "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
    "    for inp, out in zip(inputs, predictions):\n",
    "        print(inp, '->', out)\n",
    "    # im guessing bitwise_xor aint in jax yet\n",
    "    return (predictions == [onp.bitwise_xor(*inp) for inp in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "advised-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates some ran numbers to fill our weights with\n",
    "\n",
    "# this sets our network as having a hidden layer of 2 nodes\n",
    "def initial_params():\n",
    "    return [\n",
    "        onp.random.randn(3, 2),  # w1\n",
    "        onp.random.randn(3),  # b1\n",
    "        onp.random.randn(3),  # w2\n",
    "        onp.random.randn(),  #b2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-hazard",
   "metadata": {},
   "source": [
    "## finding a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moral-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to be able to find the gradient over our loss:\n",
    "loss_function = jax.grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "speaking-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent learning rate\n",
    "learning_rate = 1.\n",
    "# All possible inputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "northern-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters randomly\n",
    "params = initial_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "increased-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.63373614, -0.96423199],\n",
      "       [ 0.59448257,  0.0934048 ],\n",
      "       [-0.59233837,  1.82237451]]), array([ 0.37087304, -1.55432476, -1.78767255]), array([0.94221594, 1.16798729, 1.49574142]), -0.5188286175826627]\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "electrical-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "# as far as i can tell, this is an infinite loop\n",
    "# weird, but hey! lets follow along with the \n",
    "# tutorial for now:\n",
    "for n in itertools.count():\n",
    "    # Grab a single random input\n",
    "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
    "    # Compute the target output\n",
    "    y = onp.bitwise_xor(*x)\n",
    "    # Get the gradient of the loss for this input/output pair\n",
    "    grads = loss_function(params, x, y)\n",
    "    # Update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    # Every 100 iterations, check whether we've solved XOR\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-college",
   "metadata": {},
   "source": [
    "look, we solved it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-melbourne",
   "metadata": {},
   "source": [
    "## jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-concern",
   "metadata": {},
   "source": [
    "jax in nice cause you can jit compile code for gpus and tpus. Obviously, my mac does have those, but lets have a look anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "employed-talent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 ms ± 2.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Time the original gradient function\n",
    "\n",
    "# recall, this includes the non-jit'd grad calc\n",
    "%timeit loss_function(params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "miniature-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 µs ± 54.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run once to trigger JIT compilation\n",
    "loss_grad = jax.jit(jax.grad(loss))\n",
    "loss_function(params, x, y)\n",
    "%timeit loss_grad(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-moore",
   "metadata": {},
   "source": [
    "oh shit thats a big speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "internal-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "# retrain:\n",
    "\n",
    "params = initial_params()\n",
    "\n",
    "for n in itertools.count():\n",
    "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
    "    y = onp.bitwise_xor(*x)\n",
    "    grads = loss_grad(params, x, y)\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-still",
   "metadata": {},
   "source": [
    "## vectorising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-attribute",
   "metadata": {},
   "source": [
    "right now our network takes in 1 thing at a time. Thats not good, we want lots. Well, jax can vectorise things really easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "moved-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0), out_axes=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-alexandria",
   "metadata": {},
   "source": [
    "in_axes=(None, 0, 0), out_axes=0\n",
    "\n",
    "this is specifying where to parallelise over. We are not parallelising the first arguements (which for us in params), but we are doing it over the 0th dimension of the 2nd and 3rd (x and y)\n",
    "\n",
    "out_axis talks about the functions output, we are saying to parallelise over the loss gradients (the sole output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "skilled-characterization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "params = initial_params()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Generate a batch of inputs\n",
    "    # see, we are now using an entire batch!\n",
    "    x = inputs[onp.random.choice(inputs.shape[0], size=batch_size)]\n",
    "    y = onp.bitwise_xor(x[:, 0], x[:, 1])\n",
    "    # The call to loss_grad remains the same!\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Note that we now need to average gradients over the batch\n",
    "    params = [param - learning_rate * np.mean(grad, axis=0)\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-honduras",
   "metadata": {},
   "source": [
    "## conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-membership",
   "metadata": {},
   "source": [
    "it seems you get parallelising and jit-ing for \"free\". However, I'm not sure the use cases over TF or PyTorch - maybe if you want easy control over a gpu or tpu? I would be interested in a speedtest between this and other libraries, but maybe thats not the point, if we arent running on a g/tpu. I wonder how Autograd compares to older versions? Anyway, its got a nice numpy-y interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-mother",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
